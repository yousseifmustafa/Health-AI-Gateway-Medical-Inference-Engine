<div align="center">

# ğŸ©º SehaTech AI â€” Medical Inference Engine

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg?style=flat&logo=python&logoColor=white)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-Backend-009688.svg?style=flat&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![LangGraph](https://img.shields.io/badge/LangGraph-Orchestration-orange.svg?style=flat&logo=langchain&logoColor=white)](https://langchain-ai.github.io/langgraph/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Persistence-4169E1.svg?style=flat&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![Zilliz](https://img.shields.io/badge/Vector_DB-Zilliz_Cloud-red.svg?style=flat&logo=zilliz&logoColor=white)](https://zilliz.com/)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Models-FFD21E.svg?style=flat&logo=huggingface&logoColor=black)](https://huggingface.co/)
[![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED.svg?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)
[![Cloudinary](https://img.shields.io/badge/Cloudinary-Media_CDN-3448C5.svg?style=flat&logo=cloudinary&logoColor=white)](https://cloudinary.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=flat)](https://opensource.org/licenses/MIT)

**Production-grade Autonomous Medical Triage System** â€” featuring Self-Healing Adaptive RAG, Multi-Provider LLM Fallback, Persistent Chat History via PostgreSQL, and Agentic Sub-Graph Orchestration for safe, hallucination-resistant diagnostic support.

</div>

---

## ğŸŒŸ Project Overview

**SehaTech AI** is a hierarchical **Agentic Medical System** designed to simulate a professional clinical triage process. It acts as a centralized Supervisor Agent that intelligently routes patient queries to specialized sub-systems â€” a **Deep Diagnostic Doctor** (Self-Healing RAG), **Web Search**, **Vision Analysis** (OCR/X-Ray), or **Emergency Family Notification** â€” based on real-time intent classification.

### What Makes This Different

| Capability | Traditional Chatbot | SehaTech AI |
|---|---|---|
| Diagnosis Quality | Single LLM call, no verification | **Self-Healing RAG Loop** â€” generates, grades, retrieves evidence, re-generates |
| Memory | Lost on restart | **PostgreSQL-backed persistent chat history** with per-user thread management |
| Reliability | Single API = single point of failure | **3-Tier LLM Fallback Chain** (HuggingFace â†’ Groq â†’ Gemini) |
| Performance | Blocking, synchronous | **Async-first** with lazy-loaded models and parallel retrieval |
| Language | English-only | **Multilingual** â€” handles Egyptian Arabic, Gulf Arabic, MSA, and English natively |

---

## ğŸ—ï¸ System Architecture

### 1. Supervisor Agent â€” Tool-Based Triage Router

The Supervisor is a **Gemini 2.5 Flash**-powered LangGraph agent that classifies user intent and dispatches to specialized tools. It enforces the **"Golden Triangle"** protocol â€” collecting Symptoms, Duration, Severity, and Medical History before invoking diagnosis.

```mermaid
flowchart TD
    Start(["User Input"]) --> Router["Supervisor Agent\n Gemini 2.5 Flash + Tool Routing"]

    Router -- "Medical Symptoms" --> DoctorTool["consult_doctor_tool\n Diagnose Sub-Graph"]
    Router -- "General Info / Prices" --> WebTool["web_search_tool\n Tavily or Google Fallback"]
    Router -- "Image Uploaded" --> VisionTool["analyze_medical_image_tool\n Qwen2.5-VL / Gemini Vision"]
    Router -- "Emergency" --> FamilyTool["notify_family_tool\n SMS / Call Gateway"]

    DoctorTool --> Router
    WebTool --> Router
    VisionTool --> Router
    FamilyTool --> Router

    Router -- "Final Response" --> End(["Streamed Output via SSE"])

    DB[("PostgreSQL\nAsyncPostgresSaver")]
    Router -.-> |"checkpoint every turn"| DB
```

### 2. Self-Healing Adaptive RAG â€” The Diagnostic Engine

Unlike linear RAG pipelines, this system **generates first, judges second, and only retrieves if needed**. This "generate-then-verify" approach skips expensive vector search for simple queries while guaranteeing evidence-backed answers for complex cases.

```mermaid
flowchart TD
    Start(["Start"]) --> Generate["Generate Node\nSelf-Validating Medical LLM"]

    Generate --> GraderNode["Grade Node\nGemini Flash Lite - Structured Output"]

    GraderNode --> Router{"Score >= 0.7?\nOR\nRAG already ran?"}

    Router -- "Confident" --> End(["END - Return Answer"])
    Router -- "Needs Evidence" --> Retrieve

    subgraph RAG_Engine ["RAG Retrieval Engine"]
        direction TB
        Retrieve["Parallel Retrieval\nasyncio.gather x 3-4 queries"] --> Rerank["Cross-Encoder Reranker\nBAAI/bge-reranker-v2-m3"]
    end

    Rerank -- "Inject docs and retry" --> Generate
```

**Key design decisions:**
- **Confidence threshold = 0.7** â€” calibrated to allow strong parametric answers through while catching vague or unsafe responses
- **Loop protection** â€” RAG runs at most once; if docs are already present, the system finishes regardless of score
- **State stores text only** â€” `page_content` strings, not full `Document` objects, to prevent state serialization bloat

### 3. Multimodal Vision Pipeline

A dedicated async sub-graph for medical image analysis â€” decouples I/O-bound upload from compute-bound vision inference.

```mermaid
flowchart LR
    S(["Image + Query"]) --> UP["Upload Node\nAsync Cloudinary"]
    UP --> VIS["Vision Node\nQwen2.5-VL-7B / Gemini Fallback"]
    VIS --> E(["Analysis Result"])
    UP -.-> |"Upload Failed"| E
```

---

## âš¡ Architectural Highlights

### Singleton ModelManager â€” Load Once, Serve Forever
A **thread-safe Singleton** with Double-Checked Locking manages all LLM clients, embedding models, and rerankers. One instance serves the entire application â€” zero redundant initialization.

### Lazy-Loaded Heavy Models
The **embedding model** (Gemma-300M) and **reranker** (BGE-reranker-v2-m3) are **not loaded at startup**. They initialize on first access via `@property` decorators with thread-safe DCL, keeping cold-start time under 2 seconds.

### 3-Tier LLM Fallback Chain
Every LLM call follows: **HuggingFace â†’ Groq â†’ Gemini**. If the primary provider is rate-limited or down, the system silently switches â€” the user never sees an error. Near-100% uptime for generation.

### Async-First with `asyncio.to_thread()` Bridge
All LangGraph nodes are `async def`. Synchronous SDK calls (HuggingFace, Groq, FlagReranker) are offloaded to thread pools via `asyncio.to_thread()` â€” the event loop stays free for concurrent users.

### Persistent Chat History (PostgreSQL)
Conversations survive server restarts via **`AsyncPostgresSaver`** backed by an **`AsyncConnectionPool`** (min=2, max=10 connections). A custom `user_threads` table maps MongoDB user IDs to LangGraph thread UUIDs with **atomic upsert** to prevent race conditions.

### Parallel Vector Retrieval
Expanded queries (3-4 variations) are dispatched to Zilliz Cloud **concurrently** via `asyncio.gather()` â€” completing in the time of a single query.

### API Key Round-Robin
Each provider (HuggingFace, Google, Groq) loads multiple keys from environment variables and rotates through them, effectively multiplying the free-tier rate limit by the number of keys.

### Streaming Response (SSE)
Tokens stream to the frontend in real-time via FastAPI's `StreamingResponse` + LangGraph's `astream_events`. First token appears in ~200ms, with live Arabic status updates ("Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø®ØªØµ...").

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Role |
|:---|:---|:---|
| **Orchestration** | LangGraph (StateGraph) | Multi-agent workflows with cyclic state and conditional routing |
| **Supervisor LLM** | Gemini 2.5 Flash | Intent classification and tool dispatch |
| **Medical LLM** | II-Medical-8B (HuggingFace) | Primary diagnostic generation |
| **Grading LLM** | Gemini 2.5 Flash Lite | Structured confidence scoring |
| **Fallback LLMs** | Groq (GPT-OSS-20B), Gemini | 3-tier fallback chain |
| **Vision LLM** | Qwen2.5-VL-7B-Instruct | Medical image OCR and analysis |
| **Embeddings** | Gemma-300M (256-dim, normalized) | Semantic vector representation |
| **Reranker** | BAAI/bge-reranker-v2-m3 (FP16) | Cross-encoder relevance scoring |
| **Vector DB** | Zilliz Cloud (Managed Milvus) | Medical document retrieval |
| **Persistence** | PostgreSQL + AsyncPostgresSaver | Durable chat history checkpointing |
| **Backend** | FastAPI | Async API with SSE streaming |
| **Frontend** | Streamlit | Chat UI with image upload |
| **Image CDN** | Cloudinary | Medical image hosting |
| **Web Search** | Tavily â†’ Google Search (Fallback) | Real-time info retrieval |
| **Containerization** | Docker + docker-compose | Production deployment (12GB mem limit) |
| **Validation** | Pydantic | Structured LLM output parsing |

---

## ğŸ“ Project Structure

```
SehaTech/
â”œâ”€â”€ Langgraphs/
â”‚   â”œâ”€â”€ supervisor_graph.py      # Supervisor Agent + Lazy Factory (make_graph)
â”‚   â”œâ”€â”€ Diagnose_graph.py        # Self-Healing RAG Sub-Graph (4 nodes)
â”‚   â””â”€â”€ analyze_graph.py         # Vision Analysis Sub-Graph
â”œâ”€â”€ Models/
â”‚   â””â”€â”€ Model_Manager.py         # Singleton ModelManager + Lazy Loading + Fallback Chain
â”œâ”€â”€ Tools/
â”‚   â”œâ”€â”€ Query_Optimization_Tool.py   # Translate â†’ Rewrite â†’ Expand (Few-Shot)
â”‚   â”œâ”€â”€ parallel_retrievs_tool.py    # asyncio.gather fan-out retrieval
â”‚   â”œâ”€â”€ reranker_tool.py             # Cross-Encoder reranking
â”‚   â”œâ”€â”€ create_final_prompt_tool.py  # RAG/Memory-mode prompt builder
â”‚   â”œâ”€â”€ Post_validation_tool.py      # Post-generation QA + style matching
â”‚   â””â”€â”€ Summary_tool.py             # Rolling conversation summarization
â”œâ”€â”€ Helper/
â”‚   â”œâ”€â”€ HF_ApiManager.py         # HuggingFace key round-robin
â”‚   â”œâ”€â”€ Google_ApiManger.py      # Google key round-robin
â”‚   â”œâ”€â”€ Groq_ApiManger.py        # Groq key round-robin
â”‚   â””â”€â”€ Image_Uploader.py        # Cloudinary async upload
â”œâ”€â”€ vector_db/
â”‚   â””â”€â”€ VDB_Conection.py         # Zilliz Cloud retriever factory
â”œâ”€â”€ Database_Manager.py          # PostgreSQL pool + checkpointer + user_threads
â”œâ”€â”€ server.py                    # FastAPI streaming endpoint
â”œâ”€â”€ app.py                       # Streamlit chat UI
â”œâ”€â”€ langgraph.json               # LangGraph Studio entry points
â”œâ”€â”€ docker-compose.yml           # Production container config
â”œâ”€â”€ Dockerfile.backend           # Backend container image
â””â”€â”€ requirments.txt              # Python dependencies
```

---

## ğŸ’» Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/yousseifmustafa/Health-AI-Gateway-Medical-Inference-Engine-.git
cd Health-AI-Gateway-Medical-Inference-Engine-
```

### 2. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirments.txt
```

### 4. Setup Environment Variables

Create a `.env` file in the root directory:

```env
# --- Model Configuration ---
GROQ_MODEL_NAME=openai/gpt-oss-20b
VALIDATION_MODEL_NAME=openai/gpt-oss-20b
OPTIMIZATION_MODEL_NAME=openai/gpt-oss-20b
GENERATION_MODEL_NAME=Intelligent-Internet/II-Medical-8B
RERANKER_MODEL_NAME=BAAI/bge-reranker-v2-m3
OCR_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct
EMBEDDING_MODEL_NAME=google/embeddinggemma-300m

# --- PostgreSQL (Persistent Chat History) ---
DATABASE_URL=postgresql://user:password@host:port/database

# --- API Keys (Multiple per provider for round-robin) ---
HUGGINGFACE_API_KEY1=your_hf_key_1
HUGGINGFACE_API_KEY2=your_hf_key_2
HUGGINGFACE_API_KEY3=your_hf_key_3

GOOGLE_API_KEY1=your_google_key_1
GOOGLE_API_KEY2=your_google_key_2
GOOGLE_API_KEY3=your_google_key_3

GROQ_API_KEY1=your_groq_key_1
GROQ_API_KEY2=your_groq_key_2
GROQ_API_KEY3=your_groq_key_3

# --- Vector Database ---
ZILLIZ_URI=your_zilliz_cloud_uri
ZILLIZ_TOKEN=your_zilliz_token
ZILLIZ_COLLECTION=seha_rag_collection

# --- Web Search ---
TAVILY_API_KEY=your_tavily_key
GOOGLE_CSE_ID=your_google_cse_id
GOOGLE_API_KEY=your_google_api_key

# --- Image Hosting ---
CLOUDINARY_CLOUD_NAME=your_cloud_name
CLOUDINARY_API_KEY=your_cloudinary_key
CLOUDINARY_API_SECRET=your_cloudinary_secret
```

### 5. Run the Application

**Option A â€” Local Development:**

```bash
# Terminal 1: Start FastAPI backend
uvicorn server:app --host 0.0.0.0 --port 8000

# Terminal 2: Start Streamlit frontend
streamlit run app.py
```

**Option B â€” Docker (Production):**

```bash
docker-compose up --build
```

**Option C â€” LangGraph Studio:**

The project registers three graphs in `langgraph.json` for visual debugging:
- `supervisor_graph` â†’ Full triage agent with tools
- `Diagnose_graph` â†’ RAG diagnostic pipeline (testable independently)
- `Image_Analyzer` â†’ Vision analysis pipeline

---

## ğŸ§ª Testing Scenarios

The system has been tested against complex medical edge cases:

| Test Case | What It Proves |
|---|---|
| ğŸ§¬ **The "Fabry Disease" Challenge** | Successfully diagnosed a rare, multi-organ genetic disorder by cross-referencing symptoms across cardiology, dermatology, and nephrology via vector retrieval |
| ğŸš« **Hallucination Resistance** | Correctly refused to diagnose fabricated conditions (e.g., "Purple Hiccups Syndrome") â€” the validation layer rejected the low-confidence answer |
| ğŸš¨ **Emergency Protocol** | Automatically triggered family notification for critical symptom patterns ("Chest pain + radiating to arm") without requiring user confirmation |
| ğŸŒ **Multilingual Triage** | Accepted Egyptian Arabic slang ("Ø¹Ù†Ø¯ÙŠ ØµØ¯Ø§Ø¹ Ù†ØµÙÙŠ"), translated it for internal medical processing, and returned the diagnosis in the same dialect |
| ğŸ”„ **Self-Healing Loop** | For ambiguous symptoms, the initial LLM answer scored < 0.7, triggering RAG retrieval â†’ reranking â†’ re-generation with evidence â€” producing a verified, cited answer |
| ğŸ’¾ **Persistence Across Restarts** | Conversation history survived server restart via PostgreSQL checkpointing â€” the user resumed their session without repeating symptoms |

---

## ğŸ“ Design Principles

1. **Never Crash** â€” Every component follows the pattern: try best option â†’ fall back â†’ graceful error message
2. **Never Block** â€” All I/O is async or offloaded to thread pools; the event loop stays free
3. **Never Waste** â€” Lazy loading, conditional RAG, and round-robin keys minimize resource usage and API costs
4. **Never Forget** â€” PostgreSQL persistence ensures no conversation is lost, even across deployments
5. **Never Hallucinate** â€” The Self-Healing RAG Loop + structured confidence grading catches unsafe answers

---

<div align="center">

**Built with** â¤ï¸ **by** [Yousseif Mustafa](https://github.com/yousseifmustafa)

*SehaTech AI â€” Because in healthcare, "I don't know" is better than a wrong answer.*

</div>
